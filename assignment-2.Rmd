---
title: "Assignment 2"
author: "Nika Jurov"
date: "10/17/2018"
output: html_document
---

```{r}
`%>%` <- magrittr::`%>%`
```


```{r}
source("functions2.R")
```


# Exercise 1

```{r}
iris_subset_1 <- iris[c(89:94, 108:112),]
iris_subset_2 <- iris[88:114,]
```


``` {r, cache=T}

permuted_sub1 <- permutation_twogroups(iris_subset_1, 
                               "Sepal.Width", "Species", 
                               "versicolor", "virginica", 
                               difference_in_means)

permuted_sub2 <- permutation_twogroups(iris_subset_2, 
                                "Sepal.Width", "Species", 
                                "versicolor", "virginica", 
                                difference_in_means)


perm_sub1_tibble <- tibble::as_tibble(permuted_sub1["permuted"])
p <- ggplot2::ggplot(perm_sub1_tibble, ggplot2::aes(x=permuted)) +
  ggplot2::geom_histogram(fill="darkblue", colour="black") +
  ggplot2::xlim(-0.7, 0.7) +
  ggplot2::ylim(0, 2200) +
  ggplot2::labs(x = "Mean of permuted subset 1")
p <- p + ggplot2::geom_vline(xintercept=permuted_sub1$observed)
print(p)


perm_sub2_tibble <- tibble::as_tibble(permuted_sub2["permuted"])
p2 <- ggplot2::ggplot(perm_sub2_tibble, ggplot2::aes(x=permuted)) +
  ggplot2::geom_histogram(fill="darkgreen", colour="black") +
  ggplot2::xlim(-0.7, 0.7) +
  ggplot2::ylim(0, 2200) +
  ggplot2::labs(x = "Mean of permuted subset 2")
p2 <- p2 + ggplot2::geom_vline(xintercept=permuted_sub2$observed)
print(p2)

```

```{r}
mean_diff_sub1 <- difference_in_means(iris_subset_1, 
                                      "Sepal.Width", "Species", 
                                      "versicolor", "virginica")

mean_diff_sub2 <- difference_in_means(iris_subset_2, 
                                      "Sepal.Width", "Species",
                                      "versicolor","virginica")

```

Why there are differences in the two histograms across the two subsets? Why are there differences in the position of the observed test statistics?
Calculate a two-sided p-value for each of the two tests. Comment on the values. Why are they different?

The two different subsets are different and so are their histograms:

* `iris_subset_1` has 11 observations,  `iris_subset_2` has 27 and is thus almost three times of the size of the former. 
* The values of mean differences for permuted data are scattered wider for `iris_subset_1` than for `iris_subset_2`.
* `iris_subset_2` seems to belong more to a normal distribution than `iris_subset_1`, there are some visual gaps in the latter.
* The two-sided t.tests reject with very small p values both observeded means as the means of the permuted distribution.
* I have found help online in how to find the two sided p values, but I am unsure whether this is the correct way to do according to the instructions. The idea is to find the probability density for right and left side of some value *x* - in other words, what is the probability that some value *x* lies either above or below in the distribution. Now, we first find the z-score which is a representation of the distance of a value from the mean measured in standard deviations. Then we use `pnorm()` function integrated in `r` to find out the probability of normal distribution. For `iris_subset_1` the p value is 0.1692266 and for `iris_subset_2` the p value is 0.06822858. So the probability that the observed mean lies within the permuted distribution is somewhat closer for the  `iris_subset_2`, which is also confirmed graphically. I think it is because the standard deviation is also smaller for the second subset. Since there are very few values in the subset 1, there is higher probability that the value of this distribution lies to the right or the left of the observed mean difference. However, we might also notice that the distribution is probably not a good representation of the mean difference of the sepal width between the two species, as it simply does not contain a lot of data.
* And, for both subsets it is clear that the observed mean is quite far from the mean of the permuted data. 


```{r}
t.test(permuted_sub1$permuted,alternative = "two.sided", mu=mean_diff_sub1)
t.test(permuted_sub2$permuted,alternative = "two.sided", mu=mean_diff_sub2)

```


```{r}

p_val_sub1 <- find_two_sided(permuted_sub1, mean_diff_sub1)
p_val_sub2 <- find_two_sided(permuted_sub2, mean_diff_sub2)

print(p_val_sub1)
print(p_val_sub2)
```

# Exercise 2

```{r}
devtools::install_github("ewan/stats_course", subdir="data/stress_shift")
```




## Task A

```{r}
stress_shift_3dict <- dplyr::filter(stressshift::stress_shift_unamb, 
                                    Dict == 'W1802' | 
                                    Dict == 'J1917' | 
                                    Dict == 'C1687')

print(dplyr::count(stress_shift_3dict) == 451)
```


## Task B

```{r}
stress_shift_3dict_using_pipe <- 
  stressshift::stress_shift_unamb %>%
  dplyr::filter(
    Dict == 'W1802' | 
    Dict == 'J1917' | 
    Dict == 'C1687')
  
identical(stress_shift_3dict, stress_shift_3dict_using_pipe)
```


## Task C

```{r}
stress_shift_3dict_nouns <- 
  stress_shift_3dict %>%
  dplyr::filter(Category == "Noun")
  
stress_shift_3dict_verbs <- 
  stress_shift_3dict %>%
  dplyr::filter(Category == "Verb")

stress_shift_3dict_using_bind <-
  dplyr::bind_rows(
    stress_shift_3dict_nouns,
    stress_shift_3dict_verbs)
  
stress_shift_3dict_using_bind_reversed <-
  dplyr::bind_rows(
    stress_shift_3dict_verbs,
    stress_shift_3dict_nouns)

identical(stress_shift_3dict, stress_shift_3dict_using_bind)
identical(stress_shift_3dict, stress_shift_3dict_using_bind_reversed)
```

One table (in my code, `stress_shift_3dict_using_bind`) puts nouns first, so the *order* of values in rows is identical to `stress_shift_3dict`. The tables are the same in their values (nothing has changed but the order), because they have been concatenated differently. It depends what we do with this data, but I think in most cases the order does not matter. If we only take values out of them (meaning, we filter data by word category or by syllable etc.), the result will be the same. If we work with indices though, it matters, because each index corresponds to a row (in a hypothetical case where we do something with the table and indices, we need to know that the first half of the table is for example only verbs, so filtering by indices (unsure why one would do this but one can) from there will result in verbs only).


## Task D

```{r}
stress_shift_nouns_renamed <-
  stressshift::stress_shift_unamb %>%
  dplyr::filter(Category == "Noun") %>%
  dplyr::select(Word,
                Dict,
                Syllable) %>%
  dplyr::rename(Syllable_Noun = Syllable)


stress_shift_verbs_renamed <-
  stressshift::stress_shift_unamb %>%
  dplyr::filter(Category == "Verb") %>%
  dplyr::select(Word,
                Dict,
                Syllable) %>%
  dplyr::rename(Syllable_Verb = Syllable)


stress_shift_wide <-
  dplyr::inner_join(
    stress_shift_nouns_renamed,
    stress_shift_verbs_renamed
  )
```

The contents of `stress_shift_wide` are all the columns from both joined tables, but only the rows from `stress_shift_nouns_renamed` where they match the values in `stress_shift_verbs_renamed`. This means that the values that appear in both tables appear in the new table only if they match. Therefore, for example if there is verb present in a dictionary and the corresponding noun does not have this particular dictionary present not even once, the entry will be omitted.


## Task E

```{r}
library(RColorBrewer)
ggplot2::ggplot(stressshift::stress_shift_unamb,
                ggplot2::aes(x=Category, fill=Syllable)) +
  ggplot2::geom_bar(position="dodge", colour="black") + 
  ggplot2::scale_fill_brewer(palette="Set3")
```


## Task F

```{r}
stress_shift_byword <-
  stress_shift_wide %>%
  dplyr::group_by(Word) %>%
  dplyr::summarize(Noun_Percent_Syll_1 = 
                     (sum(Syllable_Noun == "Syllable 1") /
                         n()) * 100,
                   Verb_Percent_Syll_1=
                     (sum(Syllable_Verb == "Syllable 1") /
                         n()) * 100)
  
  
print(dplyr::count(stress_shift_byword) == 149)
```

## Task G

```{r}
ggplot2::ggplot(stress_shift_byword, 
                ggplot2::aes(
                  x=Noun_Percent_Syll_1, 
                  y=Verb_Percent_Syll_1)) +
                ggplot2::geom_point() +
                ggplot2::labs(x = "Percentage of stress on the noun",
                              y = "Percentage of stress on the verb")
```

## Task H

```{r}
stress_shift_full_join <-
  dplyr::full_join(
    stress_shift_nouns_renamed,
    stress_shift_verbs_renamed
  )

stress_shift_byword_all <-
  stress_shift_full_join %>%
  dplyr::group_by(Word) %>%
  dplyr::summarize(Noun_Percent_Syll_1 = 
                     (sum(Syllable_Noun == "Syllable 1", na.rm=TRUE) /
                         length(na.omit(Syllable_Noun)) * 100),
                   Verb_Percent_Syll_1 =
                     (sum(Syllable_Verb == "Syllable 1", na.rm=TRUE) /
                         length(na.omit(Syllable_Verb)) * 100))

```


# Exercise 3

```{r}
set.seed(12)
df_A <- data.frame("group" = "A", "value" = rnorm(50, 3, 2))
df_B <- data.frame("group" = "B", "value" = rnorm(50, 4, 2))
df <- rbind(df_A, df_B)

sample_A <- dplyr::sample_n(df_A, 5)
sample_B <- dplyr::sample_n(df_B, 5)

set.seed(NULL)
t.test(df_A$value, df_B$value)
t.test(sample_A$value,sample_B$value)

```

If we test two different data sets with a t.test, we test whether their difference means is equal to 0 (this is the null hypothesis) or not. Since the p value here (`p-value = 0.01312`) is smaller than 0.05, we can safely conclude that the difference in means between group A and group B is not the same. This makes sense as we have generated normal distributions with to different means per each group.

If we take the 5 observation per group sample, we observe that `p-value = 0.2782`. This means that we cannot reject the null hypothesis that the difference in means between the two sampled data is zero. We can conclude that this sample was too small to detect the difference in means (which we know is true as we have generated the data with different means, even though they are close).


```{r}
sample_a <- rnorm(5, 3, 2)
sample_b <- rnorm(5, 4, 2)

overall_mean <- mean(c(sample_a, sample_b))
N_SAMPLES <- 9999
statistics <- rep(NA, N_SAMPLES)
for (i in 1:N_SAMPLES) {
  sample_1_h0 <- rnorm(5, overall_mean, 2)
  sample_2_h0 <- rnorm(5, overall_mean, 2)
  v <- t.test(sample_1_h0, sample_2_h0)
  statistics[i] <- v$statistic
}
hist(statistics, freq=FALSE, breaks=40)
abline(v=t.test(sample_a, sample_b)$statistic)


overall_mean1 <- mean(c(df_A$value, df_B$value))
N_SAMPLES <- 9999
statistics1 <- rep(NA, N_SAMPLES)
for (i in 1:N_SAMPLES) {
  df_A_h0 <- rnorm(50, overall_mean, 2)
  df_B_h0 <- rnorm(50, overall_mean, 2)
  v1 <- t.test(df_A_h0, df_B_h0)
  statistics1[i] <- v1$statistic
}
hist(statistics1, freq=FALSE, breaks=80, xlim=c(-6,6))
abline(v=t.test(df_A$value, df_B$value)$statistic)

```


Looking at the observed t value and the mean t value as generated by permutations, I see that the more samples we have, the more the observed t value is away from the overall t values distribution. Which again means that it is much easier to perform a statistical test on data with more samples - when we have too little data, it is hard to prove or know whether what we are testing is or is not what we had hypothesised with null hypothesis.

We see clearly on the graphs that the observed t value lies outside the distribution of t values as generated by permutations when we have 50 instead of 5 samples (where the observed t value is very close to the permuted mean t value).

# Exercise 4

```{r}
sample_sizes <- c(5,50)
mean_diffs <- c(1,2)
st_devs <- c(2,6)

power_df <- estimate_statistical_power(sample_sizes, mean_diffs, st_devs)

```



```{r}

data_by_sample_size1 <- ggplot2::ggplot(power_df, ggplot2::aes(x =st_dev, y=power)) + 
                        ggplot2::geom_violin(ggplot2::aes(fill=sample_size), trim= FALSE) +
                        ggplot2::facet_grid(~ sample_size)
  

data_by_sample_size2 <- ggplot2::ggplot(power_df, ggplot2::aes(x = mean_diff, y=power)) + 
                        ggplot2::geom_violin(ggplot2::aes(fill=sample_size), trim= FALSE) + 
                        ggplot2::facet_grid(~ sample_size)


data_by_mean_diff <- ggplot2::ggplot(power_df, ggplot2::aes(x = sample_size, y=power)) + 
                     ggplot2::geom_violin(ggplot2::aes(fill=mean_diff), trim= FALSE) + 
                     ggplot2::facet_grid(~ mean_diff)


data_by_st_deviation <- ggplot2::ggplot(power_df, ggplot2::aes(x =sample_size, y=power)) + 
                        ggplot2::geom_violin(ggplot2::aes(fill=st_dev), trim= FALSE) + 
                        ggplot2::facet_grid(~ st_dev)

print(data_by_sample_size1)
print(data_by_sample_size1)
print(data_by_mean_diff)
print(data_by_st_deviation)
  
```

Looking at the violin plots, we can see that the sample size is very important: 5 samples consistently has lower statistical power than the 50 samples. 5 samples has a very low power, regardless of how much the standard deviation or the mean difference was.

Grouping the data by the mean difference, we might say that the higher the mean difference, the higher the power.

In my resuts, the wider the standard deviation is, the lower the statistical power is. 

So, it seems the best to have data with low standard deviation, big mean difference and many samples. This seems reasonable as by having more data the values are "reinforced" - we notice them much easier. If the mean difference is bigger in the first place it is also much easier to detect that with further statistical tests. As said before, the sample number is the one that makes - at least here in this representation - the biggest difference. The statistical power is really low for data with only 5 samples. Thinking about life in general, this seems logical, as we migh not want to extrapolate conclusions according to small number of observations. Otherwise we risk being wrong - this is also called, in more informal context, jumping to conclusions.